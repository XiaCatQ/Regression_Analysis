---
title: "EDA"
author: "Matthew Yau, ZiYing(Sophie) Chen, Xinyu Dong"
date: "2023-03-09"
output:
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
fig_height: 6
fig_width: 6
df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, resultS = FALSE, include=FALSE}
library(tidyverse)
library(psych)
library(inspectdf)
library(dplyr)
library(kableExtra)
library(knitr)
library(ggplot2)
```

## Statistical Description
There are 14 categorical and 4 numerical variables in the dataset, and our target variable is “HeartDisease”. This is a clean dataset without any missing data. Among the 319,795 observations, we removed 18,078 duplicates. Therefore, the following explanatory data analysis would only perform on 301,717 observations. 

## Univariate Analysis
- Numerical Data:  

From Table1 we can see that the means and medians of *BMI* and *SleepTime* are close, which suggests that there are no obvious skewness of the data. On the other hand, means of both *PhysicalHealth* and *MentalHealth* are bigger than their medians, which means that they skew to the right. That makes sense because most people in a survey would rather not claiming they have health issues. 

We can use the range and the standard deviation to see the spread of the data, but using histograms would be a better option. From Figure 1, we can see that most people scored their physical and mental health to be 0 which means most people did not feel bad in past month. Contrastly, some of them claimed they had physical and mental health problem every day in past month.The spreads of *PhysicalHealth* and *MentalHealth* are similar according to the charts and that makes sense because the range of scores are the same and these two variables might relate to each other.


```{r}
data <- read.csv("data/heart_2020_cleaned.csv")
# head(data)

data <- data[!duplicated(data), ]
# str(data)
num_cols <- unlist(lapply(data, is.numeric))
df_num <- data[, num_cols]
table1 <- describe(df_num) 
table1 <-kable(table1, booktabs = TRUE, caption = "Statistics for Numerical Variables") %>%
  kable_styling(latex_options="scale_down")
table1
```

```{r, fig.height = 4, fig.cap= "Histogram of Numerical Variables"}
par(mfrow=c(2,2))
hist(df_num$BMI, xlab = NULL, main = "BMI")
hist(df_num$PhysicalHealth, xlab = NULL, main = "Physical Health")
hist(df_num$MentalHealth, xlab = NULL, main = "Mental Health")
hist(df_num$SleepTime, xlab = NULL, main = "Sleep Time")
```

From Figure 2, we detected many outliers existing in all the variables, but we would not remove them for now because maybe these outliers have higher chance to get heart disease.

```{r, fig.height = 4, fig.cap= "Boxplot of Numerical Variables"}
par(mfrow=c(2,2))
boxplot(df_num$BMI, xlab = NULL, main = "BMI")
boxplot(df_num$PhysicalHealth, xlab = NULL, main = "Physical Health")
boxplot(df_num$MentalHealth, xlab = NULL, main = "Mental Health")
boxplot(df_num$SleepTime, xlab = NULL, main = "Sleep Time")
```

- Categorical Data:  

From Figure 3, We can that our target variable *HeartDisease* is unbalanced which most of the cases do not have heart disease. The *AgeCategory* and *Sex* looks balanced here, we perform $\chi^2$ test to see if there is any significant difference of the probabilities of getting heart disease among age groups and sex later. "White" category dominates the *Race*, we would proceed the analysis by convert it to binary data with "non-white" as the other. *GenHealth* indicates that most people feeling good which match the majority of the observations do not have heart disease. The rest of the variables are binary and all of them incline to one side.

```{r, fig.height = 4, fig.cap= "Frequency of Categorical Variables"}
df <- data %>% inspect_cat()  # frequency of categorical data
df %>% show_plot() 
data$Race[data$Race != "White"] <- "non-white"
```

## Bivariate Analysis

- Categorical Data

We perform $\chi^2$ test for correlation between *HeartDisease* and *AgeGroup* and *Sex* and *Race*:   

Ho: *HeartDisease* is not correlated to *AgeGroup*.  
Ha: They are correlated.  
$\alpha$ = 0.05  
The p-value is much lower than 0.05, so we reject Ho, which suggests different groups has different probabilities to get heart disease.

```{r}
age_table <- table(data$HeartDisease, data$AgeCategory)
chisq.test(xtabs(~ data$HeartDisease + data$AgeCategory), correct = F)
```

Ho: *HeartDisease* is not correlated to *Sex*.  
Ha: They are correlated.  
$\alpha$ = 0.05  
The p-value is much lower than 0.05, so we reject Ho, which suggests the probabilities to get heart disease for male and female are different.

```{r}
sex_table <- table(data$HeartDisease, data$Sex)
chisq.test(xtabs(~ data$HeartDisease + data$Sex), correct = F)
```

Ho: *HeartDisease* is not correlated to *Race*.  
Ha: They are correlated.  
$\alpha$ = 0.05  
The p-value is much lower than 0.05, so we reject Ho, which suggests the probabilities to get heart disease for white and non-white are different.

```{r}
race_table <- table(data$HeartDisease, data$Race)
chisq.test(xtabs(~ data$HeartDisease + data$Race), correct = F)
```

- Numerical Data

According to Table 2, there is no strong linear correlation among the numeric variables.
```{r}
cor_table <- cor(df_num) 
table2 <- kable(cor_table, booktabs = TRUE, caption = "Correlation Table") %>%
  kable_styling(latex_options="scale_down")
table2

# res<-cor.test(data$PhysicalHealth, data$MentalHealth, method="kendall")
# res
```

We then further analyze them in different age groups because we would love to reduce the number of age groups. *BMI* goes higher as people get older and then after 65 it gets lower. *SleepTime* reveals an uncommon phenomona that people who are 80 or older sleeps more. Figure 4 also indicates that younger generation suffer more from mental issues and older generation suffer more on physical problems.

```{r, fig.height = 4, fig.cap= "Numerical Variables among Different  Age Groups"}
par(mfrow=c(2,2))
ggplot(data = data, mapping = aes(x = AgeCategory, y = BMI)) +
  geom_boxplot()
ggplot(data = data, mapping = aes(x = AgeCategory, y = SleepTime)) + 
  geom_boxplot()
ggplot(data = data, mapping = aes(x = AgeCategory, y = PhysicalHealth)) +
  geom_boxplot()
ggplot(data = data, mapping = aes(x = AgeCategory, y = MentalHealth)) +
  geom_boxplot()
```

- Logistic Model

By fitting logistic regression, we can see that almost all the variables are significant for predicting *HeartDisease*. Surprisingly, *AlchoholDrinking* has negative association with *HeartDisease*.

Combine the result from the model and the Figure 4, we might consider to regroup the *AgeGroup* into only 4 groups (< 40, 40-59, 60-79, >80). Also, to find the vulnerable groups of getting heart disease, we may use decision tree and the PCA.
```{r}
logit_model <- glm(as.factor(HeartDisease)~., data = data, family = binomial)
summary(logit_model)
```

## What scientific questions you will try to answer:

- How those living habits and health condition-related variables may affect the possibility of being diagnosed with heart disease in general.

- How each variable contributes to the inference of heart disease, identify mostly related variables.

- What is the correlation among those variables?

- How to identify vulnerable groups getting heart disease.



## The statistical analysis techniques you will use to answer those questions (with justification)

The scientific questions above are more focused on statistical inference rather than performing predictions. To understand the relationship between those emphasized variables, we will both perform classification and regression on this dataset. 

#### Classification:

The classification is aimed at answering the vulnerable groups and how the variable will contribute to the diagnosis. The statistical method we choose is `Decision Tree Classification`, reasons are:

- **Model interpretation:** Though the decision tree may not provide the most accurate prediction, the boundary it gives is interpretable and fits well with our purpose of studying the contribution to heart disease. 
- **Built-in variable selection:** The decision tree itself can help us with variable selection, which is important cause we want to know the most related  variables 
- **Capable of dealing with mixed variable type**: Our data set is a combination of numerical and categorical data, and `Decision Trees` handle those well. 
- **A non-parametric aspect**: It will provide a non-parametric way aside from our regression method to be compared.
- **Perform well on imbalanced dataset**: The proportion of the heart disease in our data set is not balanced, but `Decision tree`'s hierarchical structure allows it to learn signals from both classes. 



#### Regression:

Here will study our data with `Logistic Regression`, which is a pretty intuitive way. 

- **Intuitive Regression Method**: Logistic regression is a widely accepted method in terms of building linear relationships between binary response and other predictors, with a range of well-developed statistical testing and evaluation techniques. 

- **Marginal Interpretation**: The advantage of using a linear combination of the coefficients is interpretable and we could compare their coefficients to understand how each variable is counting for the heart disease diagnosis.
- **Probabilistic interpretation**: Rather than just give a classification result, the response is actually probabilistic, which will enhance our statistical argumentation. 
- **Correlation Calibration**: By examining potential correlations and multilinearity among those variables, we could perform multiple trials on regression with different interaction terms. 